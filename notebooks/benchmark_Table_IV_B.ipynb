{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cabb6c5-dc3f-4006-96b6-ac9f4f21e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92255e-258d-4cf5-81c4-d6cc983d2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from aer.benchmark import benchmark, BENCHMARK_DATA, METRICS\n",
    "from orion.evaluation import contextual_confusion_matrix\n",
    "from orion.evaluation.contextual import record_observed, record_expected\n",
    "\n",
    "# Datasets\n",
    "NAB = ['artificialWithAnomaly', 'realAdExchange', 'realAWSCloudwatch', 'realTraffic', 'realTweets']\n",
    "NASA = ['MSL', 'SMAP']\n",
    "YAHOO = ['YAHOOA1', 'YAHOOA2', 'YAHOOA3', 'YAHOOA4']\n",
    "UCR = ['UCR']\n",
    "ALL_DATASETS = NAB + NASA + YAHOO + UCR\n",
    "\n",
    "RESULTS_DIRECTORY = os.path.join(os.getcwd(), 'results')\n",
    "\n",
    "# Additional Metrics\n",
    "del METRICS['accuracy']\n",
    "METRICS['confusion_matrix'] = contextual_confusion_matrix\n",
    "METRICS['observed'] = record_observed\n",
    "METRICS['expected'] = record_expected\n",
    "METRICS = {k: partial(fun, weighted=False) for k, fun in METRICS.items()}\n",
    "\n",
    "\n",
    "def run_experiment(experiment_name: str, pipelines: dict, datasets: list, metrics: dict,\n",
    "                   results_directory: str = RESULTS_DIRECTORY, workers: int = 1,\n",
    "                   tqdm_log_file: str = 'output.txt'):\n",
    "    datasets = {key: BENCHMARK_DATA[key] for key in datasets}\n",
    "    scores = benchmark(\n",
    "        pipelines=pipelines,\n",
    "        datasets=datasets,\n",
    "        metrics=metrics,\n",
    "        rank='f1',\n",
    "        show_progress=True,\n",
    "        workers=workers,\n",
    "        tqdm_log_file=tqdm_log_file\n",
    "    )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd52703-2dc3-499a-a75e-74b89112b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"AER (MULT)\"\n",
    "pipelines = {\n",
    "    'aer': 'aer_ablation-mult'\n",
    "}\n",
    "results = run_experiment(\n",
    "    experiment_name=experiment_name,\n",
    "    pipelines=pipelines,\n",
    "    datasets=ALL_DATASETS,\n",
    "    metrics=METRICS,\n",
    "    results_directory=RESULTS_DIRECTORY,\n",
    "    workers=1,\n",
    "    tqdm_log_file = f'{experiment_name}.txt'\n",
    ")\n",
    "results['pipeline'] = experiment_name\n",
    "results.to_csv(f'results/{experiment_name}_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b0e91a-bd45-425d-891c-919ec7fb9e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"AER (SUM)\"\n",
    "pipelines = {\n",
    "    'aer': 'aer_ablation-sum'\n",
    "}\n",
    "_results = run_experiment(\n",
    "    experiment_name=experiment_name,\n",
    "    pipelines=pipelines,\n",
    "    datasets=ALL_DATASETS,\n",
    "    metrics=METRICS,\n",
    "    results_directory=RESULTS_DIRECTORY,\n",
    "    workers=1,\n",
    "    tqdm_log_file = f'{experiment_name}.txt'\n",
    ")\n",
    "_results['pipeline'] = experiment_name\n",
    "_results.to_csv(f'results/{experiment_name}_results.csv', index=False)\n",
    "results = pd.concat([results, _results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a9e0f5-ba27-4655-b31b-077947fdc392",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"AER (PRED)\"\n",
    "pipelines = {\n",
    "    'aer': 'aer_ablation-pred'\n",
    "}\n",
    "_results = run_experiment(\n",
    "    experiment_name=experiment_name,\n",
    "    pipelines=pipelines,\n",
    "    datasets=ALL_DATASETS,\n",
    "    metrics=METRICS,\n",
    "    results_directory=RESULTS_DIRECTORY,\n",
    "    workers=1,\n",
    "    tqdm_log_file = f'{experiment_name}.txt'\n",
    ")\n",
    "_results['pipeline'] = experiment_name\n",
    "_results.to_csv(f'results/{experiment_name}_results.csv', index=False)\n",
    "results = pd.concat([results, _results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c7d52-34be-42a1-ba13-f3b7b9c30d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"AER (REC)\"\n",
    "pipelines = {\n",
    "    'aer': 'aer_ablation-rec'\n",
    "}\n",
    "_results = run_experiment(\n",
    "    experiment_name=experiment_name,\n",
    "    pipelines=pipelines,\n",
    "    datasets=ALL_DATASETS,\n",
    "    metrics=METRICS,\n",
    "    results_directory=RESULTS_DIRECTORY,\n",
    "    workers=1,\n",
    "    tqdm_log_file = f'{experiment_name}.txt'\n",
    ")\n",
    "_results['pipeline'] = experiment_name\n",
    "_results.to_csv(f'results/{experiment_name}_results.csv', index=False)\n",
    "results = pd.concat([results, _results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b05ce-e6eb-4f4f-a235-d0856243b4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(f'results/Table_IV_B_results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aer-paper",
   "language": "python",
   "name": "aer-paper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
